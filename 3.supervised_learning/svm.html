<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Support Vector Machines</title>
    <meta charset="utf-8" />
    <meta name="author" content="Bea Hernández" />
    <meta name="date" content="2019-03-21" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Support Vector Machines
## Supervised learning
### Bea Hernández
### 2019-03-21

---

class: inverse, center, middle

# Support Vector Machines



---
# Support Vector Machines

Support vector machines, known better as SVMs, are a machine learning model that use hyperplanes to separate data. To separate and partition our data, we find some kind of plane (or in the cases of two-dimensional data, a line) that separates them and use the vectors that maximize the separation in the data.

&lt;img src="figs/smv.png" width="50%" style="display: block; margin: auto;" /&gt;

---
# Support Vector Machines

SVMs work by employing something called the *kernel trick*: This is a method by which we can **transform** the data for which we are trying to draw a decision boundary, and then **apply a hyperplane** separation on that transformed data. In its simplest form, the kernel trick means transforming data into another dimension that has a clear dividing margin between classes of data.

In practice, this transformation is more or less a **black box** because the feature space can be quite complex, but the idea is still the same.

&lt;img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Kernel_trick_idea.svg/1000px-Kernel_trick_idea.svg.png" width="60%" style="display: block; margin: auto;" /&gt;

---
# Iris - Support Vector Machines


```r
library(e1071)

iris_2dimension &lt;- iris[, c("Petal.Length", "Petal.Width", "Species")]
index &lt;- caret::createDataPartition(iris_2dimension$Species, p = 0.3, list = F)
train &lt;- iris_2dimension[index, ]
test &lt;- iris_2dimension[-index, ]

svmfit &lt;- e1071::svm(Species ~ ., data = train, kernel = "linear", scale = T)

plot(svmfit, train)
```

&lt;img src="svm_files/figure-html/data_iris-1.png" style="display: block; margin: auto;" /&gt;

---
# Iris - Support Vector Machines


```r
tuned &lt;- tune(svm, Species ~ ., data = train, kernel = "linear",
              ranges = list(cost = 10^(-3:3)))

summary(tuned)
```

```
## 
## Parameter tuning of 'svm':
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost
##   0.1
## 
## - best performance: 0.025 
## 
## - Detailed performance results:
##    cost error dispersion
## 1 1e-03 0.680 0.18135294
## 2 1e-02 0.510 0.22335821
## 3 1e-01 0.025 0.07905694
## 4 1e+00 0.025 0.07905694
## 5 1e+01 0.025 0.07905694
## 6 1e+02 0.025 0.07905694
## 7 1e+03 0.025 0.07905694
```

---
# Iris - Support Vector Machines


```r
svmfit &lt;- svm(Species ~ ., data = train, kernel = "linear",
              cost = 0.1, scale = T)

plot(svmfit, train)
```

&lt;img src="svm_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---
class: inverse, center, middle

# k-Nearest Neighors

---
# kNN

k-nearest neighbors (kNN) is a rather simple machine learning algorithm that basically takes all the available cases in our data and predicts a target based on some kind of similarity measure—in this case, distance. We supose our neighbors to be like us, so the distance gives us a prediction.

For a regression example, the kNN algorithm calculates the average of our response variable for the kNNs. The mathematical underpinnings are the same for classification, but tweaked slightly because those are categorical instead of numeric values.

The algorithm is highly unbiased in nature and makes no prior assumption of the underlying data. Being simple and effective in nature, it is easy to implement and has gained good popularity. Indeed it is simple but kNN algorithm has drawn a lot of flake for being extremely simple! If we take a deeper look, this doesn’t create a model since there’s no abstraction process involved. Yes, the training process is really fast as the data is stored verbatim (hence lazy learner) but the prediction time is pretty high with useful insights missing at times. Therefore, building this algorithm requires time to be invested in data preparation (especially treating the missing data and categorical features) to obtain a robust model.

---
# Iris with kNN


```r
library(caret)

nor &lt;-function(x) { (x -min(x))/(max(x)-min(x)) }

index &lt;- createDataPartition(iris$Species, p = 0.7, list=F)

train &lt;- iris[index, ]
test &lt;- iris[-index, ]

prop.table(table(train$Species)) * 100
```

```
## 
##     setosa versicolor  virginica 
##   33.33333   33.33333   33.33333
```

```r
prop.table(table(iris$Species)) * 100
```

```
## 
##     setosa versicolor  virginica 
##   33.33333   33.33333   33.33333
```

---
# Iris with kNN


```r
trainX &lt;- train[, c(1,2,3,4)]
preProcValues &lt;- preProcess(x = trainX, method = c("center", "scale"))
preProcValues
```

```
## Created from 105 samples and 4 variables
## 
## Pre-processing:
##   - centered (4)
##   - ignored (0)
##   - scaled (4)
```

```r
knn &lt;- train(Species ~ ., data = train, method = "knn", 
             preProcess = c("center","scale"), tuneLength = 20)
```

--

&lt;img src="svm_files/figure-html/train-1.png" style="display: block; margin: auto;" /&gt;

---
# Iris with kNN


```r
knn_p &lt;- predict(knn, newdata = test)
confusionMatrix(knn_p, test$Species)
```

```
## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         14         1
##   virginica       0          1        14
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9556          
##                  95% CI : (0.8485, 0.9946)
##     No Information Rate : 0.3333          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9333          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.9333           0.9333
## Specificity                 1.0000            0.9667           0.9667
## Pos Pred Value              1.0000            0.9333           0.9333
## Neg Pred Value              1.0000            0.9667           0.9667
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3111           0.3111
## Detection Prevalence        0.3333            0.3333           0.3333
## Balanced Accuracy           1.0000            0.9500           0.9500
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
