<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Bea Hernández" />
    <meta name="date" content="2019-03-19" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Regression
## Supervised learning
### Bea Hernández
### 2019-03-19

---

class: inverse, center, middle

# What is regression?



---
# What is regression?

- The main motivation behind regression is to build an **equation** by which we can learn more about our data.
- There is no hard-and-fast rule about which type of regression model to fit to your data, however. 
- Investigates the relationship between a **dependent** (target) and **independent** variables (predictor).
- Types of regression:
  - Linear Regression: relationship between dependent variable and one or more independent variables using a best fit straight line
  - Polynomial Regression: the power of independent variable is more than 1
  - Lasso Regression: Least Absolute Shrinkage and Selection Operator penalizes the absolute size of the regression coefficients
  - Logistic Regression: find the probability of the event being a success or not.
  - Other variations like: Stepwise Regression, Ridge Regression, and ElasticNet Regression

---
class: inverse, center, middle

# Linear Regression

---
# Fuel efficiency with Linear Regression


```r
library(ggplot2)

ggplot(mtcars, aes(mpg, disp)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Fuel Efficiency", 
       subtitle = "Engine size (mpg) vs Fuel Efficiency (disp)") +
   geom_smooth(method='lm', level = 0)
```

&lt;img src="regression_files/figure-html/plot_mtcars-1.png" style="display: block; margin: auto;" /&gt;

---
# Fuel efficiency with Linear Regression


```r
model &lt;- lm(mpg ~ disp, data = mtcars)
summary(model)
```

```
## 
## Call:
## lm(formula = mpg ~ disp, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.8922 -2.2022 -0.9631  1.6272  7.2305 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 29.599855   1.229720  24.070  &lt; 2e-16 ***
## disp        -0.041215   0.004712  -8.747 9.38e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.251 on 30 degrees of freedom
## Multiple R-squared:  0.7183,	Adjusted R-squared:  0.709 
## F-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10
```

---
# Fuel efficiency with Linear Regression

```r
##     Min      1Q  Median      3Q     Max
## -4.8922 -2.2022 -0.9631  1.6272  7.2305
```

- Residuals: measure of vertical distance from each data point to the fitted line in our model.

--

```r
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 29.599855   1.229720  24.070  &lt; 2e-16 ***
## mtcars$disp -0.041215   0.004712  -8.747 9.38e-10 ***
```

- Coefficients: estimates for the coefficients of our linear equation. `y = 0.04x + 29.59`
- Std. Error: those coefficients come error estimates as given by the Std. Error. `y = (−0.04±0.005)x+(29.59±1.23)`
- t-value: measurement of the difference relative to the variation in our data.
- p-value: statistical assessments of significance. The workings of p-values are more complicated than that, but for our purposes a p-value being of value less than 0.05 means that we can take the number as being statistically significant.

---
# Fuel efficiency with Linear Regression

```r
## Residual standard error: 3.251 on 30 degrees of freedom
```

- Residual standard error: error estimate pertains to the standard deviation of our data.

--

```r
## Multiple R-squared:  0.7183, Adjusted R-squared:  0.709
```

- Multiple R-squared: R-squared value for when we have multiple predictors. Invariably our multiple R-squared will go up. This is because some feature we add to the model will explain some part of the variance, whether its true or not.

- Adjusted R-squared: R-squared tends to be a better representation of a model’s accuracy when there’s multiple features.

--

```r
## F-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10
```

- F-statistic: ratio of the variance explained by parameters in the model and the unexplained variance.

---
class: inverse, center, middle

# Multivariate Regression

---
# From linear to multivariate

Suppose that you want to build a more robust model of fuel efficiency with **more variables** built into it. Fuel efficiency of a vehicle can be a complex phenomenon with many contributing factors other than engine size, so finding all of the features that are responsible for driving the behavior of the model in the most accurate fashion is where you want to utilize regression as you have been, but in a multivariate context.

`y = b + m1x1`

*intercept, b, and the slope, m, tied to the one variable*

`y = b + m1x1 + m2x2 + m3x3 + (...)`

*where x1, x2, x3, and so forth, are different features in the model, such as a vehicle’s weight, engine size, number of cylinders, and so on.*

---
# Fuel efficiency - Multivariate Regression


```r
summary(lm(mpg ~ disp + wt, data = mtcars))
```

```
## 
## Call:
## lm(formula = mpg ~ disp + wt, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4087 -2.3243 -0.7683  1.7721  6.3484 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 34.96055    2.16454  16.151 4.91e-16 ***
## disp        -0.01773    0.00919  -1.929  0.06362 .  
## wt          -3.35082    1.16413  -2.878  0.00743 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.917 on 29 degrees of freedom
## Multiple R-squared:  0.7809,	Adjusted R-squared:  0.7658 
## F-statistic: 51.69 on 2 and 29 DF,  p-value: 2.744e-10
```

`disp` not statistically significant!

---
# Fuel efficiency - Multivariate Regression


```r
summary(lm(mpg ~ wt + cyl, data = mtcars))
```

```
## 
## Call:
## lm(formula = mpg ~ wt + cyl, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.2893 -1.5512 -0.4684  1.5743  6.1004 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  39.6863     1.7150  23.141  &lt; 2e-16 ***
## wt           -3.1910     0.7569  -4.216 0.000222 ***
## cyl          -1.5078     0.4147  -3.636 0.001064 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.568 on 29 degrees of freedom
## Multiple R-squared:  0.8302,	Adjusted R-squared:  0.8185 
## F-statistic: 70.91 on 2 and 29 DF,  p-value: 6.809e-12
```

Substitute the `disp` for an statistically significant feature.

---
# Take into account

### Feature engineering

**Pick features** to add to the model one at a time and cut the ones that are statistically insignificant.

### Regularization

Keep all of the features but mathematically **reduce the coefficients** of the less important ones to minimize their impact on the model.

---
class: inverse, center, middle

# Lasso regression

---
# Regularization with lasso

Let’s see what the linear model for the `mtcars` dataset would look like if we included all the features. We would have an equation like this:

```
mpg = 12.3 − 0.11cyl + 0.01disp − 0.02hp + 0.79drat − 3.72wt + 0.82qsec + 0.31vs 
      + 2.42am + 0.66gear − 0.20carb
```

According to this linear equation, fuel efficiency is most sensitive to the weight of the vehicle (`-3.72wt`), given that this one has the largest coefficient. However, most of these are all within an order of magnitude or so to one another. Regularization would keep all of the features, but the less important ones would have their coefficients scaled down much further.

---
# Regularization with lasso


```r
library(dplyr)
library(glmnet)
y &lt;- mtcars %&gt;% select(mpg) %&gt;% scale(center = TRUE, scale = FALSE) %&gt;% as.matrix()
X &lt;- mtcars %&gt;% select(-mpg) %&gt;% as.matrix()

lasso_cv &lt;- cv.glmnet(X, y, alpha = 1, standardize = TRUE)
coef(lasso_cv)
```

```
## 11 x 1 sparse Matrix of class "dgCMatrix"
##                        1
## (Intercept) 13.380766879
## cyl         -0.833427049
## disp         .          
## hp          -0.005886408
## drat         .          
## wt          -2.287815523
## qsec         .          
## vs           .          
## am           .          
## gear         .          
## carb         .
```

---
class: inverse, center, middle

# Polynomial regression

---
# US Population - Polynomial regression

Polynomial regression is simply fitting a **higher degree** function to the data.


```r
population &lt;- data.frame(uspop) %&gt;%
  mutate(us_pop = as.numeric(uspop),
         year = seq(from = 1790, to = 1970, by = 10))

ggplot(population, aes(x = year, y = us_pop)) +
  geom_point() +
  geom_smooth(method = 'lm', level = 0)
```

&lt;img src="regression_files/figure-html/polynomial-1.png" style="display: block; margin: auto;" /&gt;

---
# US Population - Polynomial regression


```r
lm1 &lt;- lm(us_pop ~ poly(year, 1), data = population)
summary(lm1)
```

```
## 
## Call:
## lm(formula = us_pop ~ poly(year, 1), data = population)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -19.569 -14.776  -2.933   9.501  36.345 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     69.769      4.158   16.78 5.16e-12 ***
## poly(year, 1)  257.542     18.125   14.21 7.29e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 18.12 on 17 degrees of freedom
## Multiple R-squared:  0.9223,	Adjusted R-squared:  0.9178 
## F-statistic: 201.9 on 1 and 17 DF,  p-value: 7.286e-11
```

---
# US Population - Polynomial regression


```r
lm2 &lt;- lm(us_pop ~ poly(year, 2), data = population)
summary(lm2)
```

```
## 
## Call:
## lm(formula = us_pop ~ poly(year, 2), data = population)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.5997 -0.7105  0.2669  1.4065  3.9879 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     69.7695     0.6377  109.40  &lt; 2e-16 ***
## poly(year, 2)1 257.5420     2.7798   92.65  &lt; 2e-16 ***
## poly(year, 2)2  73.8974     2.7798   26.58 1.14e-14 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.78 on 16 degrees of freedom
## Multiple R-squared:  0.9983,	Adjusted R-squared:  0.9981 
## F-statistic:  4645 on 2 and 16 DF,  p-value: &lt; 2.2e-16
```

---
# US Population - Polynomial regression


```r
ggplot(population, aes(x = year, y = us_pop)) +
  geom_point() +
  geom_smooth(method = 'lm', level = 0, formula=y ~ poly(x, 2, raw=TRUE))
```

&lt;img src="regression_files/figure-html/lm2_plot-1.png" style="display: block; margin: auto;" /&gt;

---
# US Population - Polynomial regression


```r
uspop.2020 &lt;- data.frame(year = c(1980, 1990, 2000, 2010), 
                         us_pop = c(226.5,249.6, 282.2, 309.3))

lm1_p &lt;- predict(lm1, uspop.2020)
lm2_p &lt;- predict(lm2, uspop.2020)

caret::postResample(lm1_p, uspop.2020$us_pop)
```

```
##       RMSE   Rsquared        MAE 
## 75.6224454  0.9961522 73.0772018
```

```r
caret::postResample(lm2_p, uspop.2020$us_pop)
```

```
##      RMSE  Rsquared       MAE 
## 8.1923114 0.9970722 7.4107013
```

&lt;img src="regression_files/figure-html/residuals-1.png" style="display: block; margin: auto;" /&gt;

---
class: inverse, center, middle

# Logistic regression

---
# Stock prediction with logistic regression


```r
library(ISLR)

head(Smarket, n = 1)
```

```
##   Year  Lag1   Lag2   Lag3   Lag4 Lag5 Volume Today Direction
## 1 2001 0.381 -0.192 -2.624 -1.055 5.01 1.1913 0.959        Up
```

```r
summary(Smarket)
```

```
##       Year           Lag1                Lag2          
##  Min.   :2001   Min.   :-4.922000   Min.   :-4.922000  
##  1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500  
##  Median :2003   Median : 0.039000   Median : 0.039000  
##  Mean   :2003   Mean   : 0.003834   Mean   : 0.003919  
##  3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750  
##  Max.   :2005   Max.   : 5.733000   Max.   : 5.733000  
##       Lag3                Lag4                Lag5         
##  Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.92200  
##  1st Qu.:-0.640000   1st Qu.:-0.640000   1st Qu.:-0.64000  
##  Median : 0.038500   Median : 0.038500   Median : 0.03850  
##  Mean   : 0.001716   Mean   : 0.001636   Mean   : 0.00561  
##  3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.59700  
##  Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.73300  
##      Volume           Today           Direction 
##  Min.   :0.3561   Min.   :-4.922000   Down:602  
##  1st Qu.:1.2574   1st Qu.:-0.639500   Up  :648  
##  Median :1.4229   Median : 0.038500             
##  Mean   :1.4783   Mean   : 0.003138             
##  3rd Qu.:1.6417   3rd Qu.: 0.596750             
##  Max.   :3.1525   Max.   : 5.733000
```

---
# Stock prediction with logistic regression


```r
ggplot(stack(Smarket %&gt;% select(-Year)), aes(x = ind, y = values)) +
  geom_boxplot() +
  theme_minimal()
```

&lt;img src="regression_files/figure-html/boxplot_logistic-1.png" style="display: block; margin: auto;" /&gt;

---
# Stock prediction with logistic regression


```r
library(corrplot)
correlations &lt;- cor(Smarket %&gt;%
                      mutate(direction_up = ifelse(Direction == 'Up', 1, 0)) %&gt;%
                      select(-Direction))
corrplot(correlations, method="shade")
```

&lt;img src="regression_files/figure-html/correlations-1.png" style="display: block; margin: auto;" /&gt;
Volume and year are highly correlated, as well as Today and our target variable.

---
# Stock prediction with logistic regression


```r
train &lt;- Smarket %&gt;% filter(Year &lt; 2005)
test &lt;- Smarket %&gt;% filter(Year &gt;= 2005)

library(caret)
```

```
## Loading required package: lattice
```

```r
glm.fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume + Today, 
               data = train, family = binomial)
coef(glm.fit)
```

```
##  (Intercept)         Lag1         Lag2         Lag3         Lag4 
## -101.2261049   -4.0007986   13.3194066   -0.7816574    4.0827435 
##         Lag5       Volume        Today 
##   12.7554422   84.4569698 2794.2451377
```

```r
glm.probs &lt;- predict(glm.fit, newdata = test, type = "response")
glm.pred &lt;- ifelse(glm.probs &gt; 0.5, "Up", "Down")
table(glm.pred, test$Direction)
```

```
##         
## glm.pred Down  Up
##     Down  110   0
##     Up      1 141
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
