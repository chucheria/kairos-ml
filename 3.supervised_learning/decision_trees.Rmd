---
title: "Tree-Based Methods"
subtitle: "Supervised learning"
author: "Bea Hern√°ndez"
date: "`r Sys.Date()`"
output: 
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, center, middle

# Tree-Based Methods

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, eval = FALSE, message = FALSE,
       fig.retina = 3)
```

---
# Decision trees

Decision tree is a type of supervised learning algorithm that can be used in both regression and classification problems. Tree-based models can be a lot  intuitive.

```{r, fig.align='center', echo=F, out.width = "45%"}
knitr::include_graphics('http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1528907338/decision-tree_c2yyos.png')
```

--

- **Root Node** represents the entire population or sample.
- **Splitting** is a process of dividing a node into two or more sub-nodes.
- When a sub-node splits into further sub-nodes, it is called a **Decision Node.**
- Nodes that do not split is called a **Terminal Node** or a **Leaf**.
- When you remove sub-nodes of a decision node, this process is called **Pruning**.
- A sub-section of an entire tree is called **Branch**.
- A node, which is divided into sub-nodes is called a **parent** node of the sub-nodes; whereas the sub-nodes are called the **child** of the parent node.

---
# Decision trees

```{r, echo=F}
weather <- tibble::tribble(
    ~Week, ~Sky_condition, ~Wind_speed, ~Humidity, ~Good_result,
        1,       "cloudy",       "low",    "high",        "yes",
        2,        "rainy",       "low",  "normal",        "yes",
        3,        "sunny",      "high",  "normal",        "yes",
        4,       "cloudy",      "high",    "high",        "yes",
        5,       "cloudy",       "low",  "normal",        "yes",
        6,        "rainy",      "high",    "high",         "no",
        7,        "rainy",      "high",  "normal",         "no",
        8,       "cloudy",      "high",  "normal",        "yes",
        9,        "sunny",       "low",    "high",         "no",
       10,        "sunny",       "low",  "normal",        "yes",
       11,        "rainy",       "low",  "normal",        "yes",
       12,        "sunny",       "low",    "high",         "no",
       13,        "sunny",      "high",    "high",         "no"
    )
head(weather)
```

The tree subsets the data by certain criteria and then builds a tree so that when we have new data, it follows the branches of the tree to a result.

```{r, fig.align='center', echo=F, out.width = "60%"}
knitr::include_graphics('figs/decision-tree-2.png')
```

---
# Splitting the trees

You begin by looking at how a decision tree works by splitting on the variable *Sky Condition*, but why did we choose that one? Why not choose a different variable to split on instead? A decision tree wants to maximize the *purity* of its splits.

```{r, fig.align='center', echo=F, out.width = "60%"}
knitr::include_graphics('figs/decision-splitting.png')
```

---
# Iris with Random Forest

```{r split_tree, message=F, warning = F, fig.align='center', fig.height=4, fig.width=11}
library(randomForest)

rf <- randomForest(Species ~ ., data = iris, importance = TRUE, 
                   proximity = TRUE, as.tree = T, ntree=500)

#devtools::install_github('araastat/reprtree')
reprtree:::plot.getTree(rf)
```

---
# Pros and cons

The major advantage of using decision trees is that they are intuitively **very easy to explain**. They can be displayed graphically, and they can easily handle qualitative predictors *without* the need to create dummy variables.

However, decision trees generally do not have the same level of predictive accuracy as other approaches, since **they aren't quite robust**. A small change in the data can cause a large change in the final estimated tree. Decision trees are prone to **overfitting**, especially when a tree is particularly deep. This is due to the amount of specificity we look at leading to smaller sample of events that meet the previous assumptions.

**Random forest** builds multiple decision trees and merges them together to get a more accurate and stable prediction.

```{r, fig.align='center', echo=F, out.width = "45%"}
knitr::include_graphics('https://qph.fs.quoracdn.net/main-qimg-55d780e747ab1c9decd7f3828cc684a9')
```

---
class: inverse, center, middle

# Gradient Boosted Trees and XGBoost

---
# Gradient Boosted Trees and XGBoost

### Gradient Boosted Trees

When building a decision tree, a challenge is to decide how to split a current leaf. A *greedy* way to do this is to consider every possible split on the remaining features and calculate the new error for each split; you could then pick the tree which most reduces your loss.

### XGBoost

XGBoost stands for eXtreme Gradient Boosting. XGBoost is one of the fastest implementations of gradient boosted trees.

It does this by tackling one of the major inefficiencies of gradient boosted trees: considering the potential loss for all possible splits to create a new branch (especially if you consider the case where there are thousands of features, and therefore thousands of possible splits)

Although XGBoost implements a few regularization tricks, this speed up is by far the most useful feature of the library, allowing many hyperparameter settings to be investigated quickly. This is helpful because there are many, many hyperparameters to tune.