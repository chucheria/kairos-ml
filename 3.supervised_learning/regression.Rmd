---
title: "Supervised learning"
subtitle: "Intro to ML"
author: "Bea Hernández"
date: "`r Sys.Date()`"
output: 
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, center, middle

# Regression

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, eval = FALSE, message = FALSE,
       fig.retina = 3)
```

---
# What is regression?

- The main motivation behind regression is to build an equation by which we can learn more about our data.
- There is no hard-and-fast rule about which type of regression model to fit to your data, however. Choosing between a logistic regression, linear regression, or multivariate regression model depends on the problem and the data that you have.

---
class: inverse, center, middle

# Linear Regression

---
# An example model

```{r plot_mtcars, message=F, fig.height=4}
library(ggplot2)

ggplot(mtcars, aes(mpg, disp)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Fuel Efficiency", 
       subtitle = "Engine size (mpg) vs Fuel Efficiency (disp)") +
   geom_smooth(method='lm', level = 0)
```

---
# An example model

```{r model_cars, message=F}
model <- lm(mtcars$mpg ~ mtcars$disp)
summary(model)
```

---
# An example model

```r
lm(formula = mtcars$mpg ~ mtcars$disp)
```

- Call: function call we used.

```r
##     Min      1Q  Median      3Q     Max
## -4.8922 -2.2022 -0.9631  1.6272  7.2305
```

- Residuals: measure of vertical distance from each data point to the fitted line in our model.

```r
##              Estimate Std. Error t value Pr(>|t|)
## (Intercept) 29.599855   1.229720  24.070  < 2e-16 ***
## mtcars$disp -0.041215   0.004712  -8.747 9.38e-10 ***
```

- Coefficients: estimates for the coefficients of our linear equation. `y = 0.04x + 29.59`
- Std. Error: those coefficients come error estimates as given by the Std. Error. `y = (−0.04±0.005)x+(29.59±1.23)`
- t-value: measurement of the difference relative to the variation in our data.
- p-value: statistical assessments of significance. The workings of p-values are more complicated than that, but for our purposes a p-value being of value less than 0.05 means that we can take the number as being statistically significant.

---
# An example model

```r
## Residual standard error: 3.251 on 30 degrees of freedom
```

- Residual standard error: error estimate pertains to the standard deviation of our data.

```r
## Multiple R-squared:  0.7183, Adjusted R-squared:  0.709
```

- Multiple R-squared: R-squared value for when we have multiple predictors. Invariably our multiple R-squared will go up. This is because some feature we add to the model will explain some part of the variance, whether its true or not.

- Adjusted R-squared: R-squared tends to be a better representation of a model’s accuracy when there’s multiple features.

```r
## F-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10
```

- F-statistic: ratio of the variance explained by parameters in the model and the unexplained variance.

---
class: inverse, center, middle

# Multivariate Regression

---
# From linear to multivariate

Suppose that you want to build a more robust model of fuel efficiency with more variables built into it. Fuel efficiency of a vehicle can be a complex phenomenon with many contributing factors other than engine size, so finding all of the features that are responsible for driving the behavior of the model in the most accurate fashion is where you want to utilize regression as you have been, but in a multivariate context.

`y = b + m1x1`

*intercept, b, and the slope, m, tied to the one variable*

`y = b + m1x1 + m2x2 + m3x3 + (...)`

*where x1, x2, x3, and so forth, are different features in the model, such as a vehicle’s weight, engine size, number of cylinders, and so on.*

---
# An example model

```{r multivariate}
summary(lm(mpg ~ disp + wt, data = mtcars))
```

`disp` not statistically significant!

---
# An example model

```{r multi_3}
summary(lm(mpg ~ wt + cyl, data = mtcars))
```

Substitute the `disp` for an statistically significant feature.

---
# Take into account

### Careful selection of features

Pick features to add to the model one at a time and cut the ones that are statistically insignificant.

### Regularization

Keep all of the features but mathematically reduce the coefficients of the less important ones to minimize their impact on the model.

---
# Regularization

Through the use of regularization, you can make your model more succinct and reduce the noise in the dataset that might be coming from features that have little impact on what you are trying to model against.

Let’s see what the linear model for the mtcars dataset would look like if we included all the features. We would have an equation like this:

```
mpg = 12.3 − 0.11cyl + 0.01disp − 0.02hp + 0.79drat − 3.72wt 
      + 0.82qsec + 0.31vs + 2.42am + 0.66gear − 0.20carb
```

According to this linear equation, fuel efficiency is most sensitive to the weight of the vehicle (-3.72wt), given that this one has the largest coefficient. However, most of these are all within an order of magnitude or so to one another. Regularization would keep all of the features, but the less important ones would have their coefficients scaled down much further.