---
title: "Regression"
subtitle: "Supervised learning"
author: "Bea Hernández"
date: "`r Sys.Date()`"
output: 
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, center, middle

# What is regression?

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, eval = FALSE, message = FALSE,
       fig.retina = 3)
```

---
# What is regression?

- The main motivation behind regression is to build an **equation** by which we can learn more about our data.
- There is no hard-and-fast rule about which type of regression model to fit to your data, however. 
- Investigates the relationship between a **dependent** (target) and **independent** variables (predictor).
- Types of regression:
  - *Linear*: relationship between dependent variable and one or more independent variables using a best fit straight line
  - *Polynomial*: the power of independent variable is more than 1
  - *Lasso*: Least Absolute Shrinkage and Selection Operator penalizes the absolute size of the regression coefficients
  - *Logistic*: find the probability of the event being a success or not.
  - Other variations like *Stepwise* Regression, *Ridge* Regression, and *ElasticNet* Regression

---
class: inverse, center, middle

# Linear Regression

---
# Fuel efficiency with Linear Regression

```{r plot_mtcars, message=F, fig.height=4, fig.align='center'}
library(ggplot2)

ggplot(mtcars, aes(mpg, disp)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Fuel Efficiency", 
       subtitle = "Engine size (mpg) vs Fuel Efficiency (disp)") +
   geom_smooth(method='lm', level = 0)
```

---
# Fuel efficiency with Linear Regression

```{r model_cars, message=F}
model <- lm(mpg ~ disp, data = mtcars)
summary(model)
```

---
# Fuel efficiency with Linear Regression

```r
##     Min      1Q  Median      3Q     Max
## -4.8922 -2.2022 -0.9631  1.6272  7.2305
```

- Residuals: measure of vertical distance from each data point to the fitted line in our model.

--

```r
##              Estimate Std. Error t value Pr(>|t|)
## (Intercept) 29.599855   1.229720  24.070  < 2e-16 ***
## mtcars$disp -0.041215   0.004712  -8.747 9.38e-10 ***
```

- Coefficients: estimates for the coefficients of our linear equation. `y = 0.04x + 29.59`
- Std. Error: those coefficients come error estimates as given by the Std. Error. `y = (−0.04±0.005)x+(29.59±1.23)`
- t-value: measurement of the difference relative to the variation in our data.
- p-value: statistical assessments of significance. The workings of p-values are more complicated than that, but for our purposes a p-value being of value less than 0.05 means that we can take the number as being statistically significant.

---
# Fuel efficiency with Linear Regression

```r
## Residual standard error: 3.251 on 30 degrees of freedom
```

- Residual standard error: error estimate pertains to the standard deviation of our data.

--

```r
## Multiple R-squared:  0.7183, Adjusted R-squared:  0.709
```

- Multiple R-squared: R-squared value for when we have multiple predictors. Invariably our multiple R-squared will go up. This is because some feature we add to the model will explain some part of the variance, whether its true or not.

- Adjusted R-squared: R-squared tends to be a better representation of a model’s accuracy when there’s multiple features.

--

```r
## F-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10
```

- F-statistic: ratio of the variance explained by parameters in the model and the unexplained variance.

---
class: inverse, center, middle

# Multivariate Regression

---
# From linear to multivariate

Suppose that you want to build a more robust model of fuel efficiency with **more variables** built into it. Fuel efficiency of a vehicle can be a complex phenomenon with many contributing factors other than engine size, so finding all of the features that are responsible for driving the behavior of the model in the most accurate fashion is where you want to utilize regression as you have been, but in a multivariate context.

`y = b + m1x1`

*intercept, b, and the slope, m, tied to the one variable*

`y = b + m1x1 + m2x2 + m3x3 + (...)`

*where x1, x2, x3, and so forth, are different features in the model, such as a vehicle’s weight, engine size, number of cylinders, and so on.*

---
# Fuel efficiency - Multivariate Regression

```{r multivariate}
summary(lm(mpg ~ disp + wt, data = mtcars))
```

`disp` not statistically significant!

---
# Fuel efficiency - Multivariate Regression

```{r multi_3}
summary(lm(mpg ~ wt + cyl, data = mtcars))
```

Substitute the `disp` for an statistically significant feature.

---
# Take into account

### Feature engineering

**Pick features** to add to the model one at a time and cut the ones that are statistically insignificant.

### Regularization

Keep all of the features but mathematically **reduce the coefficients** of the less important ones to minimize their impact on the model.

---
class: inverse, center, middle

# Lasso regression

---
# Regularization with lasso

Let’s see what the linear model for the `mtcars` dataset would look like if we included all the features. We would have an equation like this:

```
mpg = 12.3 − 0.11cyl + 0.01disp − 0.02hp + 0.79drat − 3.72wt + 0.82qsec + 0.31vs 
      + 2.42am + 0.66gear − 0.20carb
```

According to this linear equation, fuel efficiency is most sensitive to the weight of the vehicle (`-3.72wt`), given that this one has the largest coefficient. However, most of these are all within an order of magnitude or so to one another. Regularization would keep all of the features, but the less important ones would have their coefficients scaled down much further.

---
# Regularization with lasso

```{r lasso, message=F}
library(dplyr)
library(glmnet)
y <- mtcars %>% select(mpg) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X <- mtcars %>% select(-mpg) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()

lasso_cv <- cv.glmnet(X, y, alpha = 1, standardize = TRUE)
coef(lasso_cv)
```

---
class: inverse, center, middle

# Polynomial regression

---
# US Population - Polynomial regression

Polynomial regression is simply fitting a **higher degree** function to the data.

```{r polynomial, message=F, fig.align='center', fig.height=4}
population <- data.frame(uspop) %>%
  mutate(us_pop = as.numeric(uspop),
         year = seq(from = 1790, to = 1970, by = 10))

ggplot(population, aes(x = year, y = us_pop)) +
  geom_point() +
  geom_smooth(method = 'lm', level = 0)
```

---
# US Population - Polynomial regression

```{r lm1}
lm1 <- lm(us_pop ~ poly(year, 1), data = population)
summary(lm1)
```

---
# US Population - Polynomial regression

```{r lm2}
lm2 <- lm(us_pop ~ poly(year, 2), data = population)
summary(lm2)
```

---
# US Population - Polynomial regression

```{r lm2_plot, message=F, fig.align='center', fig.height=5}
ggplot(population, aes(x = year, y = us_pop)) +
  geom_point() +
  geom_smooth(method = 'lm', level = 0, formula=y ~ poly(x, 2, raw=TRUE))
```

---
# US Population - Polynomial regression

```{r errors, message=F, fig.align='center', fig.height=4}
uspop.2020 <- data.frame(year = c(1980, 1990, 2000, 2010), 
                         us_pop = c(226.5,249.6, 282.2, 309.3))

lm1_p <- predict(lm1, uspop.2020)
lm2_p <- predict(lm2, uspop.2020)

caret::postResample(lm1_p, uspop.2020$us_pop)
caret::postResample(lm2_p, uspop.2020$us_pop)

```

```{r residuals, echo = F, fig.align='center', fig.height=3}
par(mfrow = c(1, 2))
plot(resid(lm1), main = "Degree 1", xlab = "", ylab = "Fit Residual")
plot(resid(lm2), main = "Degree 2", xlab = "", ylab = "Fit Residual")
```

---
class: inverse, center, middle

# Logistic regression

---
# Stock prediction with logistic regression

```{r logistic_data, message=F, warning = F, fig.align='center', fig.height=4, fig.width=11}
library(ISLR)

head(Smarket, n = 1)
ggplot(stack(Smarket %>% select(-Year)), aes(x = ind, y = values)) +
  geom_boxplot() +
  theme_minimal()
```

---
# Stock prediction with logistic regression

```{r correlations, message=F, warning = F, fig.align='center', fig.height=4, fig.width=11}
library(corrplot)
correlations <- cor(Smarket %>%
                      mutate(direction_up = ifelse(Direction == 'Up', 1, 0)) %>%
                      select(-Direction))
corrplot(correlations, method="shade")
```
Volume and year are highly correlated, as well as Today and our target variable.

---
# Stock prediction with logistic regression

```{r model_logistic, message = F, warning=F}
train <- Smarket %>% filter(Year < 2005)
test <- Smarket %>% filter(Year >= 2005)

library(caret)
glm.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume + Today, 
               data = train, family = binomial)
coef(glm.fit)

glm.probs <- predict(glm.fit, newdata = test, type = "response")
glm.pred <- ifelse(glm.probs > 0.5, "Up", "Down")
table(glm.pred, test$Direction)
```
