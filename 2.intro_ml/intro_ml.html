<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>What is R?</title>
    <meta charset="utf-8" />
    <meta name="author" content="Bea Hernández" />
    <meta name="date" content="2019-03-12" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# What is R?
## Thinking with R
### Bea Hernández
### 2019-03-12

---

class: inverse, center, middle

# Intro to Machine Learning



---
class: middle

Machine learning (ML) is: 

- The scientific study of algorithms and statistical models that computer systems use to effectively perform a specific task **without** using explicit instructions.
- It is seen as a **subset** of artificial intelligence.&lt;sup&gt;1&lt;/sup&gt;
- Deep Learning is a **subset** of ML based on Artificial Neural Networks.

.footnote[
[1] [Wikipedia](https://en.wikipedia.org/wiki/Machine_learning)
]

---
# Types

## Types of ML 

- **Supervised learning**: the learning algorithm is presented with labelled example inputs, where the labels indicate the desired output. SML itself is composed of **classification**, where the output is categorical, and **regression**, where the output is numerical.
- **Unsupervised learning**, no labels are provided, and the learning algorithm focuses solely on detecting structure in unlabelled input data.

Note that there are also *semi-supervised learning* approaches that use labelled data to inform unsupervised learning on the unlabelled data to identify and annotate new classes in the dataset.

--

## Types of problems

- **Regression**: our target variable takes continuous values.
- **Classification**: our target variable takes categorical values.

This will shape the way we sample our data, the algoritms we choose, and the error measurement.

---
class: inverse, center, middle

# Let's model

---

We are going to do a little (more than tested) model to get to know what will come next. To do that we are going to use the `caret` package, so go ahead and install it. There's some kind of error installing `caret` in OSX, please visit [this question in SO](https://stackoverflow.com/questions/25990296/how-to-include-omp-h-in-os-x/) to fix it.

--

The dataset will be the `iris` dataset stored in base R. The dataset gives the measurements in centimeters of the variables *sepal length* and *width*, and *petal length* and *width*, for 50 flowers from each of 3 species of iris: *Iris setosa*, *versicolor*, and *virginica*.


```r
library(caret)
library(ggplot2)
summary(iris)
```

```
##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## 
```

---

We are going to split the data in *train* and *test*. With the *train* dataset we will *teach* our model for test it later on the unseen *test* dataset.

--


```r
set.seed(42)
ind &lt;- sample(c(TRUE, FALSE), nrow(iris), replace=TRUE, prob=c(0.8, 0.2))
train &lt;- iris[ind, ]
test &lt;- iris[!ind, ]


ggplot(data = reshape2::melt(train), aes(variable, value)) +
  geom_boxplot()
```

&lt;img src="intro_ml_files/figure-html/split-1.png" style="display: block; margin: auto;" /&gt;

---


```r
ggplot(data = train, aes(Species)) +
  geom_bar()
```

&lt;img src="intro_ml_files/figure-html/distribution-1.png" style="display: block; margin: auto;" /&gt;

Looks like my train set has more of one *Specie* (target variable) than of others, the previous method will be useful in other cases (to reduce bias, for example), in this case we want to have more or less the same number of each *Specie* to have a nice prediction.

---


```r
index &lt;- createDataPartition(iris$Species, p=0.20, list=FALSE)
train &lt;- iris[index, ]
test &lt;- iris[-index, ]

ggplot(data = train, aes(Species)) +
  geom_bar()
```

&lt;img src="intro_ml_files/figure-html/resplit-1.png" style="display: block; margin: auto;" /&gt;

---

Let's build our model, because we want to predict the specie given the other parameters, it is a classification problem.

We will see two metrics to evaluate the model:

- Accuracy: percentage of correctly classifies instances out of all instances.
- Kappa: like classification accuracy, except that it is normalized at the baseline of random chance on your dataset.

---


```r
model_fit &lt;- train(Species~., data = train, method = "knn", tuneGrid = expand.grid(k = 1:25))
model_fit
```

```
## k-Nearest Neighbors 
## 
## 30 samples
##  4 predictor
##  3 classes: 'setosa', 'versicolor', 'virginica' 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 30, 30, 30, 30, 30, 30, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa    
##    1  0.9362075  0.9038543
##    2  0.8904567  0.8309492
##    3  0.8894528  0.8309430
##    4  0.8829438  0.8214104
##    5  0.8643036  0.7939159
##    6  0.8368040  0.7524351
##    7  0.8413634  0.7607826
##    8  0.8344543  0.7488672
##    9  0.8039392  0.7055045
##   10  0.7845553  0.6807141
##   11  0.7406833  0.6175420
##   12  0.7332614  0.6193882
##   13  0.6754231  0.5483213
##   14  0.6193713  0.4736484
##   15  0.6003719  0.4541724
##   16  0.5441737  0.3828605
##   17  0.5115171  0.3303565
##   18  0.5115171  0.3312615
##   19  0.4755402  0.2890081
##   20  0.4539818  0.2555390
##   21  0.4140222  0.2069040
##   22  0.4168674  0.2098222
##   23  0.3839280  0.1764055
##   24  0.3579078  0.1389960
##   25  0.3507096  0.1343096
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 1.
```

---

Now it's the time to predict on the "unknown" *test* dataset:

--


```r
predictions &lt;- predict(model_fit, test)
confusionMatrix(predictions, test$Species)
```

```
## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         40          0         0
##   versicolor      0         35         0
##   virginica       0          5        40
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9583          
##                  95% CI : (0.9054, 0.9863)
##     No Information Rate : 0.3333          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9375          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.8750           1.0000
## Specificity                 1.0000            1.0000           0.9375
## Pos Pred Value              1.0000            1.0000           0.8889
## Neg Pred Value              1.0000            0.9412           1.0000
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.2917           0.3333
## Detection Prevalence        0.3333            0.2917           0.3750
## Balanced Accuracy           1.0000            0.9375           0.9688
```

---
class: inverse, center, middle

# Training and testing

---
# Training and testing

Machine learning requires us to first **train** a data model. We have data for which we want a prediction, so we pass it through the model and get a result out. Then, we evaluate the results and see the associated errors.

When building a predictive model, you need to go through **phases of validation** to ensure that you can trust its results. You need a way to see what the errors generated by the model will be so that you can **better tune** it appropriately.

*You want to predict the value for some stock price tomorrow for which it’s impossible to get the data, you could build a model that is trained on data from a few days ago and test it on data that you have from yesterday. Because you already have the answers for stock prices from yesterday, seeing what the model outputs and comparing the numbers can provide valuable feedback to see whether the model is working.*

We typically do a **70/30** split or **80/20** split (depending on the size of out dataset) of the data into the training/test subsets.

---
# Splitting the data

There are two major assumptions that we work with when doing these training/test splits:

- The data is a fair representation of the actual processes that you want to model.
- The processes that you want to model are relatively stable over time and that a model built with last month’s data should accurately reflect next month’s data.

--

## Roles of Training and Test Sets

When you split the data into a training and test set, it’s the training set that that you use for model training. The specific coefficients you get as a result of the modeling procedures are entirely based on the training dataset and don’t depend at all on the test data.

The role of the test set of data is to see how well that model stacks up against real data.

--

### Why Make a Test Set?

It is a solid way of validating that model. From a statistical standpoint, any model that gives you 100.0% accuracy should be cause for concern. You can use the test set to evaluate the predictive performance of the trees in the data to find the one with the lowest error.

---
# Resampling

Can I do the sampling again if I'm not happy with the model? **Of course**.

Doing feature selection is very important but choosing a right sample is too. One that is unbiased, that doesn't overfit/underfit our data, and that is unvariance is the *ideal* train set (almost never achiavable, we will see that in the *bias-variance tradeoff*).

---
class: inverse, center, middle

# How do I measure my model?

---
# Regression

*RMSE - Root mean square error* and *MAE - Mean absolute error* are two different ways of look at the average difference between the model output and the values we have in our test set. 

*RRSE - Root relative squared error*, *RAE - Relative Absolute Error* we measure in percentage how mucho the feature deviates sfrom its average value. They are relative errors.

&lt;img src="https://raw.githubusercontent.com/chucheria/kairos-ml/master/2.intro_ml/figs/rmse.png?token=AHVv5N-4Dj5Vwv09l61gWhUN9hXnE86fks5ch1KIwA%3D%3D" width="40%" /&gt;&lt;img src="https://raw.githubusercontent.com/chucheria/kairos-ml/master/2.intro_ml/figs/mae.png?token=AHVv5NgCZlyIfhpkoKFtfA1qZuoV2NJ7ks5ch1GawA%3D%3D" width="40%" /&gt;&lt;img src="https://raw.githubusercontent.com/chucheria/kairos-ml/master/2.intro_ml/figs/rrse.png?token=AHVv5EfKcRa9qMOjVPQCoMkUHB-A-haNks5ch1HfwA%3D%3D" width="40%" /&gt;&lt;img src="https://raw.githubusercontent.com/chucheria/kairos-ml/master/2.intro_ml/figs/rae.png?token=AHVv5IdraxTUdVylwjd06IPwiY_5E0PEks5ch1IHwA%3D%3D" width="40%" /&gt;


---
# Classification

We calculate the *confussion matrix*, in which the model output predicted classes are compared to the actual classes and the count of the model output in the cells of the matrix. This informs you of the *true positives*, *true negatives*, *false positives*, *false negatives*. This numbers allow us to get metrics on the model: 

- *Sensitivity or recall*: When it is actually the positive result, how often does it predict correctly?
- *Specificity*, When it is actually the negative result, how often does it predict correctly?
- *Precision*: When it predicts the positive result, how often is it correct?
- *Accuracy*: How often is the classifier correct?
- *F1 Score*: Harmonic mean of precision and recall.

![](https://raw.githubusercontent.com/chucheria/kairos-ml/master/2.intro_ml/figs/confussionmatrix.png?token=AHVv5LABCEhoI5ieR0E0BZAoAgxduUHvks5ch1KtwA%3D%3D)&lt;!-- --&gt;

---
class: inverse, center, middle

# Cross-validation

---
# Cross-validation

The process of training and testing data is still somewhat limited, however. In one capacity, when you’re testing the model output against the reserved data, you are seeing only what the error is for that exact grouping of the test data.

**Cross-validation** is a statistical technique by which you take your entire dataset, split it into a number of small train/test chunks, evaluate the error for each chunk, and then average those final errors. This approach winds up being a more accurate way of assessing whether your modeling approach has any issues that could be hidden in various combinations of the training and test parts of the dataset.

&lt;img src="https://raw.githubusercontent.com/chucheria/kairos-ml/master/2.intro_ml/figs/k-fold.png?token=AHVv5EmUy76QTDrX00HJAbgCpLyEnUU2ks5ch1MWwA%3D%3D" width="40%" /&gt;

---
# k-fold Cross-validation

1. Take your dataset and split it into k chunks.
2. For each of these chunks:
  1. Split the data into a smaller train/test set.
  2. Evaluate that individual chunk’s error. 
3. After you have all the errors for all the chunks, take the average.

---
class: inverse, center, middle

# Before the model

---

We are going to perform an **Exploratory Data Analysis** on the *Titanic dataset* to later do **Feature selection**.

EDA helps in:

- Gain familiarity with the dataset
- Identify feature distribution
- Identify missing values
- Do feature selection

EDA includes between others:

- Knowing the dimension of the dataset
- Checking the variables
- Looking for and filling missing values
- Looking for outliers or weird observations
- Checking the distribution of the variables
- Cleaning the strings in the dataset

---
# Load libraries and data


```r
library(tidyverse)
```

--


```r
train &lt;- read_csv('data/titanic/train.csv')
test &lt;- read_csv('data/titanic/test.csv')
```

---

```r
head(train, n = 2)
```

```
## # A tibble: 2 x 12
##   PassengerId Survived Pclass Name  Sex     Age SibSp Parch Ticket  Fare
##         &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;
## 1           1        0      3 Brau… male     22     1     0 A/5 2…  7.25
## 2           2        1      1 Cumi… fema…    38     1     0 PC 17… 71.3 
## # … with 2 more variables: Cabin &lt;chr&gt;, Embarked &lt;chr&gt;
```

```r
dim(train)
```

```
## [1] 891  12
```

```r
head(test, n = 2)
```

```
## # A tibble: 2 x 11
##   PassengerId Pclass Name  Sex     Age SibSp Parch Ticket  Fare Cabin
##         &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;
## 1         892      3 Kell… male   34.5     0     0 330911  7.83 &lt;NA&gt; 
## 2         893      3 Wilk… fema…  47       1     0 363272  7    &lt;NA&gt; 
## # … with 1 more variable: Embarked &lt;chr&gt;
```

```r
dim(test)
```

```
## [1] 418  11
```
---
# Check for nulls


```r
map_df(train, ~sum(is.na(.)))
```

```
## # A tibble: 1 x 12
##   PassengerId Survived Pclass  Name   Sex   Age SibSp Parch Ticket  Fare
##         &lt;int&gt;    &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;
## 1           0        0      0     0     0   177     0     0      0     0
## # … with 2 more variables: Cabin &lt;int&gt;, Embarked &lt;int&gt;
```

```r
map_df(test, ~sum(is.na(.)))
```

```
## # A tibble: 1 x 11
##   PassengerId Pclass  Name   Sex   Age SibSp Parch Ticket  Fare Cabin
##         &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1           0      0     0     0    86     0     0      0     1   327
## # … with 1 more variable: Embarked &lt;int&gt;
```

There's a lot of cabin information missing. If we take a look at the [data information](https://www.kaggle.com/c/titanic/data) and [looking](https://www.kaggle.com/c/titanic/discussion/4693) for the exact meaning on the *cabin* values, the first class had the top decks, it makes sense that the people towards the top were closer to the lifeboats, ergo more likely to survive. So we will keep it as predictor for now even if there's a lot of data missing.

---
# Check for correlation between variables

We are going to check the *train* dataset because is the one with the target value, and we want to have an image on the impact of each feature on the target.

But

**What is correlation?**

---
# Correlation

**Correlation** is a statistical measure (expressed as a number) that describes the size and direction of a relationship between two or more variables. 

Imagine the weather getting warmer and the ice-cream sales getting higher, if we plot both variables we will see a line with a positive slope, we can say both variables are correlated.

![](https://www.mathsisfun.com/data/images/correlation-examples.svg)

This is the definition of the *Pearson's correlation coefficient*, which is a *linear* correlation between *continuous* variables.

## What does this mean?

- One variable **might** cause the other
- They *may* be influence by other variable
- *Could* be random

---
# Correlation

Correlation between *categorical* variables can be done with *Chi-Squared* test of independence: we call two variables independent if the probability distribution of one variables is nor affected by the presence of another.

Let's see this in an example with the column target *Survived* and the *Pclass* which indicates the passenger class. Our hypothesis is that the variables are independent, we will check the *p-value* to test that hypotheses.


```r
chisq.test(as.factor(train$Pclass), as.factor(train$Survived))
```

```
## 
## 	Pearson's Chi-squared test
## 
## data:  as.factor(train$Pclass) and as.factor(train$Survived)
## X-squared = 102.89, df = 2, p-value &lt; 2.2e-16
```
The *p-value* is almost 0, this means that the probability that both variables are independent is almost 0, so they are highly correlated. Is this too subtle? Well... yeah. Not all correlation is easy to prove. But to have a visual look at the correlation we can plot and see.

---
# Correlation


```r
ggplot(train, aes(Pclass)) +
  geom_bar(aes(fill = as.factor(Survived)))
```

&lt;img src="intro_ml_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---
class: inverse, center, middle

# Exploratory data analysis 

---
# Filling missing data

So, now we know how to do correlation between two variables we can do something else from the prior list. 

Filling the missing data is one of the most *personal* tasks of the EDA. We are making up data so our observation is complete and not descarted in the model or that our new complete data is giving different information. 


```r
map_df(train, ~sum(is.na(.)))
```

```
## # A tibble: 1 x 12
##   PassengerId Survived Pclass  Name   Sex   Age SibSp Parch Ticket  Fare
##         &lt;int&gt;    &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;
## 1           0        0      0     0     0   177     0     0      0     0
## # … with 2 more variables: Cabin &lt;int&gt;, Embarked &lt;int&gt;
```

We are going to fill *Age* and *Embarked* with median value, so it doesn't aggregate too much weight. The problem is *Cabin*, we said before the Cabin would determine how closer were you to the surface, but the truth is that *Pclass* gives us a similar information, being 1st class the closer to the surface.

---


```r
library(magrittr)
train %&lt;&gt;% 
  mutate(Age = ifelse(is.na(Age), median(Age, na.rm = TRUE), Age), 
          Embarked = ifelse(is.na(Embarked), median(Embarked, na.rm = TRUE), Embarked)) %&gt;%
  select(-Cabin)
```

---

# Feature selection

When presented data with very high dimensionality, models usually choke because:

- **Training time** increases exponentially with number of features.
- Models have increasing risk of **overfitting**&lt;sup&gt;1&lt;sup&gt; with increasing number of features.

**Feature Selection** methods helps with these problems by **reducing the dimensions** without much loss of the total information. It also helps to **make sense** of the features and its importance.

.footnote[
[1] We will learn *overfitting* eventually.
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
